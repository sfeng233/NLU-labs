{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b28a7-a120-4a05-b0c9-d3ed1399ce5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# NLU+ 2021-2022: Lab 2 - Distributed Representations\n",
    "#### Authors: Christos Baziotis, Lexi Birch, Frank Keller\n",
    "\n",
    "In this lab, you will be working with pretrained\n",
    "word embeddings using NumPy. Using a small text classification dataset and pretrained word embeddings,\n",
    "you will do simple vector arithmetic operations, produce text representations for each example in the text data and do simple analysis and visualizations.\n",
    "\n",
    "##### Time Management Notes\n",
    " - The lab is long. If you get stuck in a question please ask for help from the demonstrators.\n",
    " - You don't have to answer all the questions, you may skip a question if you are running out of time and work on them later on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efda9c",
   "metadata": {},
   "source": [
    "The snippet below contains some package imports and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3209c9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(\n",
    "    suppress=True)  # suppresses the use of scientific notation for small numbers\n",
    "\n",
    "\n",
    "# you may use this function to print a numpy array and its properties\n",
    "def print_array(arr):\n",
    "    print(arr)\n",
    "    print(\"shape:\", arr.shape)\n",
    "    print(\"type:\", arr.dtype.type)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77185d2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Pretrained Word Embeddings\n",
    "\n",
    "First, we will load the 50d GloVe pretrained word embeddings (https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    " - `embeddings` is the 2D matrix that holds all the word embeddings\n",
    " - `word2id` is a dictionary mapping each word to the index (row) it occurs in the `embeddings` matrix\n",
    " - `id2word` is a dictionary mapping each index (of the `embeddings` matrix) to the corresponding word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed6050f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "Done!\n",
      "[[ 0.41800001  0.24968    -0.41242    ... -0.18411    -0.11514\n",
      "  -0.78580999]\n",
      " [ 0.013441    0.23682    -0.16899    ... -0.56656998  0.044691\n",
      "   0.30392   ]\n",
      " [ 0.15164     0.30177    -0.16763    ... -0.35652     0.016413\n",
      "   0.10216   ]\n",
      " ...\n",
      " [ 0.072617   -0.51393002  0.47279999 ... -0.18907    -0.59021002\n",
      "   0.55558997]\n",
      " [ 0.00088203  0.00020008  0.00048937 ...  0.00038875 -0.00080695\n",
      "  -0.00010637]\n",
      " [-0.04477333  0.01934512 -0.02554026 ...  0.08929352  0.0063456\n",
      "   0.02009947]]\n",
      "shape: (400002, 50)\n",
      "type: <class 'numpy.float64'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_glove(file):\n",
    "    np.random.seed(0)\n",
    "\n",
    "    _word2id = {}\n",
    "    _id2word = {}\n",
    "    _embs = []\n",
    "    print(\"Loading embeddings...\")\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            _word2id[word] = i\n",
    "            _id2word[i] = word\n",
    "            _embs.append(np.asarray(values[1:], dtype='float32'))\n",
    "\n",
    "    # <PAD> - this token is used for artifically increasing the length of a sentence\n",
    "    _embs.append(np.random.normal(loc=0, scale=0.0005, size=(len(values[1:]),)))\n",
    "    _word2id[\"<PAD>\"] = i + 1\n",
    "    _id2word[i + 1] = \"<PAD>\"\n",
    "\n",
    "    # <UNK> - this token is used when a word does not exist in our vocabulary\n",
    "    _embs.append(np.random.normal(loc=0, scale=0.05, size=(len(values[1:]),)))\n",
    "    _word2id[\"<UNK>\"] = i + 2\n",
    "    _id2word[i + 2] = \"<UNK>\"\n",
    "\n",
    "    _embs = np.array(_embs)\n",
    "    print(\"Done!\")\n",
    "    print_array(_embs)\n",
    "    return _embs, _word2id, _id2word\n",
    "\n",
    "\n",
    "EMBEDDING_PATH = 'glove.6B.50d.txt'\n",
    "embeddings, word2id, id2word = load_glove(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5269d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13075\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(word2id[\"hello\"])\n",
    "print(id2word[85])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3738f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 1. Word Vector Similarity\n",
    "\n",
    "Similar words are located near to each other in the embedding space.\n",
    "\n",
    "**Question 1.1:** Implement a function, that given a word, it will retrieve its word embedding, find its `N` closest word embeddings to it, and finally print the corresponding words.\n",
    "Use the dot product (i.e., `np.dot`) as the distance metric. The function should *not* return the original (input) word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad597d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_dot(w: str, top: int = 10):\n",
    "    emb_str = embeddings[word2id[w]]\n",
    "    distance = {}\n",
    "    for i,row in enumerate(embeddings):\n",
    "        if row != emb_str:\n",
    "            if len(topN) < 10:\n",
    "                topN[np.dot(emb_str, row)] = id2word[i]\n",
    "            else:\n",
    "                if topN[]\n",
    "                \n",
    "\n",
    "for word in [\"scotland\", \"edinburgh\", \"university\", \"informatics\", \"neural\"]:\n",
    "    print(\"\\n\",\"-\"*10, word, \"-\"*10)\n",
    "    print(most_similar_dot(word, top=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e542d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question 1.2:** Update the function to use the cosine similarity as metric, which is defined as:\n",
    "$$\n",
    "\\cos(u,v)=\\frac{u\\cdot v}{\\left\\|u\\right\\|\\left\\|v\\right\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8859228",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_cosine(w: str, top: int = 10):\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "\n",
    "\n",
    "for word in [\"scotland\", \"edinburgh\", \"university\", \"informatics\", \"neural\"]:\n",
    "    print(\"\\n\",\"-\"*10, word, \"-\"*10)\n",
    "    print(most_similar_cosine(word, top=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7db235",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While both metrics mostly return similar results, there are some differences. Cosine similarity should be less noisy than the dot product. Think about why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b4c0e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Answer\n",
    "The cosine is independent of vector length. It has been shown that the norm (i.e., length) of vectors encode information related to word frequency and not semantic or syntactic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f138e6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Question 2. Word Analogies\n",
    "\n",
    "Word embeddings capture semantic and syntactic information. This is reflected in how they are organized in the embedding space.\n",
    "Specifically, a nice property of word embeddings is that _sometimes_ they exhibit the ability to solve analogies, using simple vector arithmetic operations.\n",
    "\n",
    "In the word analogy task, we are trying to find the word (embedding) that completes (solves) the following sentence (equation)  \"*word_1* is to *word_2*, as *word_3* is to **???**\". A famous example is \"*man* is to *woman*, as *king* is to **queen**\".\n",
    "\n",
    "Mathematically, this is formulated as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "&\\vec{woman} - \\vec{man} \\approx \\vec{queen} - \\vec{king}  \\\\\n",
    "\\Rightarrow & \\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "![Word2Vec Analogies](https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg)\n",
    "image source: https://developers.google.com/machine-learning/crash-course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e5058",
   "metadata": {},
   "source": [
    "**Question 2.1** Implement the `word_analogy()` function, which will accept a positive and a negative word list and will form a query vector out of these. The function should return the most similar words to the query vector. The list of negative words can be empty. Use the cosine similarity as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e696c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def word_analogy(positive: List[str] = [], negative: List[str] = [],\n",
    "                 top: int = 3):\n",
    "    \"\"\"\n",
    "    This function implements the word analogy task (e.g., king-man+woman=?).\n",
    "    Args:\n",
    "        positive: list of words with positive sign\n",
    "        negative: list of words with negative sign\n",
    "        top: the N most similar words to return\n",
    "\n",
    "    Returns:\n",
    "        List of the top N most similar words\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "\n",
    "\n",
    "# First, let's check the result of the analogy in the example above\n",
    "print(word_analogy(positive=[\"king\", \"woman\"], negative=[\"man\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8678de4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let's see how the result(s) vary as we change the input query\n",
    "print(word_analogy(positive=[\"king\"]))\n",
    "print(word_analogy(positive=[\"king\", \"woman\"]))\n",
    "print(word_analogy(positive=[\"king\", \"woman\"], negative=[\"man\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd705f",
   "metadata": {},
   "source": [
    "**Question 2.2** Using the `word_analogy()` function and the equation in Question-2 description, answer the analogy questions below.\n",
    "\n",
    "The result should show that the embeddings produce meaningful answers for both sematic and syntactic analogies. However, they don't get everything right. Can you think of some reasons?\n",
    "\n",
    "Feel free to add/test more analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87feb029",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # semantic analogies\n",
    "    ('greece', 'athens', 'iraq'),\n",
    "    ('italy', 'italian', 'spain'),\n",
    "    ('india', 'delhi', 'japan'),\n",
    "    ('man', 'woman', 'boy'),\n",
    "\n",
    "    # grammatical analogies\n",
    "    ('small', 'smaller', 'large'),\n",
    "    ('happy', 'happiest', 'sad'),\n",
    "    ('scotland', 'scottish', 'greece'),\n",
    "    ('feeding', 'fed', 'sitting'),\n",
    "    ('good', 'awesome', 'bad'),\n",
    "    # the results below are not as expected\n",
    "    ('good', 'perfect', 'bad'),\n",
    "    ('scotland', 'edinburgh', 'england'),\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    # word1 is to word2, as word3 is to ?\n",
    "    result = ...\n",
    "\n",
    "    print('\"{}\" -> \"{}\", as \"{}\" -> \"{}\"'.format(*q, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dc343",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reasons why analogies can \"fail\"\n",
    "\n",
    " - The embeddings were not trained on sufficient data to learn good representations, in order to be able to solve that particular analogy\n",
    " - In the example we used small embeddings with only 50 dimensions, which prevents them from encoding fine-grained information\n",
    "\n",
    "Also, some of the analogies may actually make sense, even if unexpected. For example:\n",
    "`\"scotland\" -> \"edinburgh\", as \"england\" -> \"oxford\"` makes sense if the analogy is \"most important university\" instead of \"capital\". Therefore, the subtraction produces *an* analogy, but not necessarily the one we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d346d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 2.3 Polysemous words (Go to Q3 if running our of time)\n",
    "Polysemous words ([wiki](https://en.wikipedia.org/wiki/Polysemy)) have more than one meaning. Such an example is the word `java`, which can has three meanings:\n",
    " - The island in Indonesian\n",
    " - A type of coffee bean\n",
    " - A programming language\n",
    "\n",
    "Using this word as an example, and with the help of the `word_analogy` function, try to isolate, or bring out, each of these meanings.\n",
    "\n",
    "Also, explore other polysemous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14a298",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's see how we can isolate the meaning of polysemous words\n",
    "print(\"java=\", word_analogy(positive=[\"java\"], top=50), \"\\n\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Write your solution here\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e5dab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Try another polysemous word\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Write your solution here\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88c8fb",
   "metadata": {},
   "source": [
    "**Optional**: Can you try and solve an analogy (like in Question 2.2) with one of these polysemous words? Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84adf83e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# word analogy with polysemous words\n",
    "questions = [\n",
    "    ('bird', 'pigeon', 'dog'),  # this should make sense\n",
    "    ('snake', 'python', 'dog'),  # this should not make sense\n",
    "]\n",
    "for q in questions:\n",
    "    result = word_analogy(positive=[q[1], q[2]], negative=[q[0]])[0]\n",
    "    print('\"{}\" -> \"{}\", as \"{}\" -> \"{}\"'.format(*q, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb795696",
   "metadata": {},
   "source": [
    "### Text Corpus Datasets\n",
    "\n",
    "Next, we will use the DBpedia ontology text classification dataset. The data are extracted from Wikipedia articles organized into different classes. It contains 560,000 training samples and 70,000 testing samples for each of 14 non-overlapping classes from DBpedia 2014. We are going to use a subset of the test data for the (analysis) purposes of this lab.\n",
    "\n",
    "The classes are:\n",
    "\n",
    "- Company\n",
    "\n",
    "- Educational Institution\n",
    "\n",
    "- Artist\n",
    "\n",
    "- Athlete\n",
    "\n",
    "- OfficeHolder\n",
    "\n",
    "- Mean Of Transportation\n",
    "\n",
    "- Building\n",
    "\n",
    "- Natural Place\n",
    "\n",
    "- Village\n",
    "\n",
    "- Animal\n",
    "\n",
    "- Plant\n",
    "\n",
    "- Album\n",
    "\n",
    "- Film\n",
    "\n",
    "- Written Work\n",
    "\n",
    "The data are stored in a csv file, and each row (record) is structured as follows:\n",
    "\n",
    " - class: denoting the class (1-14)\n",
    "\n",
    " - title: title of the article\n",
    "\n",
    " - description: the contents of the article\n",
    "\n",
    "__Source__\n",
    "https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\n",
    "https://wiki.dbpedia.org/\n",
    "https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a9141",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# let's load the text data\n",
    "with open('dbpedia_csv/test.csv', 'r') as f:\n",
    "    data = list(csv.reader(f, delimiter=\",\"))\n",
    "data = np.array(data)\n",
    "classes = data[:, 0].astype(int)\n",
    "titles = [list(word_tokenize(x.lower())) for x in data[:, 1]]\n",
    "texts = [list(word_tokenize(x.lower())) for x in data[:, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629f250",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's inspect the contents of each variable\n",
    "print(\"classes:\", classes[:5], \"\\n\")\n",
    "print(\"titles:\", titles[:5], \"\\n\")\n",
    "print(\"texts:\", texts[:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca18e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Document/Sentence Representations\n",
    "\n",
    "Using the pretrained embeddings you will compute different text representations for a random batch of sentences. To start with, you can use the `batchify_sentences()` function below, to convert a batch of text sentences into a batch with word-ids. You can uncomment the printing commands to inspect what each step of the function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120affef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "# create a batch of random sentences\n",
    "def batchify_sentences(sents):\n",
    "    \"\"\"\n",
    "    Transforms a list of sentences (word lists) in a 2D batch,\n",
    "    by mapping each word into its word-id (i.e., row in embeddings matrix)\n",
    "    \"\"\"\n",
    "    # 1 - map words to ids\n",
    "    _ids = [[word2id.get(w, word2id[\"<UNK>\"]) for w in s] for s in sents]\n",
    "    # for x in _ids:\n",
    "    #     print(x)\n",
    "\n",
    "    # 2 - apply zero padding to make the sentences in the batch have the same length\n",
    "    _ids = np.array(list(zip_longest(*_ids, fillvalue=word2id[\"<PAD>\"]))).T\n",
    "    # print_array(_ids)\n",
    "\n",
    "    # 3 - construct some helper variables\n",
    "    _lengths = np.array([len(x) for x in sents])\n",
    "    # print_array(lengths)\n",
    "    _mask = np.array(_ids != word2id[\"<PAD>\"]).astype(int)\n",
    "    # print_array(mask)\n",
    "\n",
    "    return _ids, _lengths, _mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2e52c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's sample some sentences from the text corpus and transform them into a batch of ids. The `batchify_sentences` function returns:\n",
    "\n",
    " - A 2D `int` array with the word ids of each text sample, padded with `<PAD>` tokens, based on the length of the longest sentence in each batch.\n",
    " - A 1D `int` array (vector) with the **real** length (in words) of each text sample.\n",
    " - A 2D `boolean` array indicating whether each word is padded or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c097d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "np.random.seed(1)  # for reproducibility\n",
    "ids = set(np.random.choice(len(texts), BATCH_SIZE, replace=False))\n",
    "batch = [texts[i] for i in ids]\n",
    "# for x in batch:\n",
    "#     print(x)\n",
    "batch_ids, batch_lengths, batch_mask = batchify_sentences(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7c554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's inspect what is in each variable\n",
    "print(\"batch_ids\", batch_ids.shape, '\\n', batch_ids)\n",
    "print(\"batch_lengths\", batch_lengths.shape, '\\n', batch_lengths)\n",
    "print(\"batch_mask\", batch_mask.shape, '\\n', batch_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4e531",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.1 Vectorize the batch\n",
    "\n",
    "Convert the 2D `batch_ids` array into a 3D tensor, by replacing each word id with the corresponding word embedding.\n",
    "\n",
    "Do not use for loops. Each operation is independent therefore they can be run in parallel.\n",
    "\n",
    "Think about mapping a function onto each element of the `batch` array.\n",
    "\n",
    "The expected shape is `(5, 78, 50)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb91159",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vectorize the batch\n",
    "def vectorize_batch(batch_idx):\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    return ...\n",
    "\n",
    "\n",
    "batch_vec = vectorize_batch(batch_ids)\n",
    "print_array(batch_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6943b71",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3.2 Max-Pooling\n",
    "Compute sentence representations using max-pooling (i.e., maximum over each embedding dimension). The output shape should be `(5, 50)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94145f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Write your solution here\n",
    "# --------------------------------------------------\n",
    "max_pooling = ...\n",
    "print_array(max_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94006b",
   "metadata": {},
   "source": [
    "#### 3.3 Mean-Pooling (Average-Pooling)\n",
    "Compute sentence representations using mean-pooling. This means computing the centroid of each sentence (average of its word embeddings).\n",
    "\n",
    "Do not use for loops.\n",
    "\n",
    "Each sentence has different length! Take that into account when computing the average. You should use `batch_lengths` and `batch_mask`.\n",
    "\n",
    "The output shape should be `(5, 50)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78843113",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_pooling(inputs, lengths, masks):\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "    out = ...\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "print_array(mean_pooling(batch_vec, batch_lengths, batch_mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb853f2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Inspection of Document Embeddings\n",
    "In the previous question, you implemented a function that creates a single vector representation (i.e., embedding) out of its word embeddings. In this question you will use these text representations to inspect what information they encode.\n",
    "\n",
    "Note that, since we construct text embeddings using simple arithmetic operations out of their constituent word embedding, then the text embeddings themselves will ``live'' in the same embedding space, and as a result, it will allow us to make comparisons between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2501d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question 4.1** Create a single vector representation for each text sample in the data.\n",
    "Construct a 2D `text_embeddings` array with shape `(70000, 50)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d74d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_embeddings = []\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch = texts[i:i + batch_size]\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "\n",
    "text_embeddings = np.concatenate(text_embeddings, axis=0)\n",
    "print_array(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6712fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Question 4.2** We can create a \"concept\" vector out of a list of words, using its word embeddings. For example the list of words `[\"tomato\", \"pasta\", \"olive\"]` can produce a concept vector related to food. We can construct a vector representation for a list of words in exactly the same way as for regular sentences.\n",
    "\n",
    "Find the most similar sentences to a concept, defined as a list of words. You need to construct a vector representation out of the words of the concept and compare it with the vector representations of each text example in the data. You may choose any method you want for producing a single vector representation out of a a sequence of embeddings. In the snippet below, we already provide 3 concepts, but you may add more if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9682864",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "concepts = [\n",
    "    (\"edinburgh\", \"scotland\"),\n",
    "    (\"tv\", \"cinema\", \"movie\", \"entertainment\"),\n",
    "    (\"tomato\", \"pasta\", \"olive\"),\n",
    "]\n",
    "\n",
    "for concept in concepts:\n",
    "    print(\"\\n\",\"-\"*10, concept, \"-\"*10)\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # return the ids of the most similar words, excluding the input\n",
    "    ids = ...\n",
    "    for i in ids[:5]:\n",
    "        print(\"[{}]\".format(i), data[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf82b9a",
   "metadata": {},
   "source": [
    "**Question 4.3:** Construct a single representation for each *class* in the dataset and then print the `50` most representative words of that class.\n",
    "\n",
    "- Think what is the best way to construct the class representation?\n",
    "- If the results that you get contain a lot of generic (mostly function) words, what can you do to improve the results? Think about what you learnt about the polysemous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5fb74",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = np.array(classes)\n",
    "class_labels = [x.strip() for x in open('dbpedia_csv/classes.txt', 'r').readlines()]\n",
    "\n",
    "for cls, label in enumerate(class_labels, 1):\n",
    "    print(\"\\n\",\"-\"*10, cls, label, \"-\"*10)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Write your solution here\n",
    "    # --------------------------------------------------\n",
    "    words = ...\n",
    "    print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19f0f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. Visualization\n",
    "\n",
    "A usual step of an exploratory analysis is to visualize the data to gain insights. Here, you will create a simple plot with the purpose of inspecting how the document (i.e., the content of each article) representations are dispersed in the embedding space, according to their classes.\n",
    "\n",
    "To do this, you will have to use a dimensionality reduction technique, in order to project the document representations to 2 dimensions, and to be able to plot them. While dimensionality reduction techniques results in loss of information, they can be use as tool to build intuition. Different dimensionality reduction techniques have different pros and cons.\n",
    "\n",
    "Here, we will use `UMAP`, which is a fast non-linear dimensionality reduction algorithm. The code below computes the 2D document representations. For details in tweaking UMAP, see https://umap-learn.readthedocs.io/en/latest/parameters.html\n",
    "\n",
    "##### References\n",
    " - McInnes, L, Healy, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, ArXiv e-prints 1802.03426, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94d29b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2022)\n",
    "\n",
    "def project_2d(original_embeddings, n_neighbors=50, min_dist=0.25, metric='euclidean'):\n",
    "    fit = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=2,\n",
    "        metric=metric\n",
    "    )\n",
    "    return fit.fit_transform(original_embeddings)\n",
    "\n",
    "text_embeddings_2d = project_2d(text_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78e84e",
   "metadata": {},
   "source": [
    "**Question 5.1** Create a scatter plot using document embeddings and color-code each point based on its class. To avoid clutter (i.e., many points on top of each other) and make the plot more presentable, plot only a subset of points per class (e.g., 500).\n",
    "\n",
    "(Double-click to zoom in the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4122f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# --------------------------------------------------\n",
    "# Write your solution here\n",
    "# --------------------------------------------------\n",
    "\n",
    "# use different markers to differentiate similar colors\n",
    "markers = [\"*\",  \">\", \".\", ] \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
